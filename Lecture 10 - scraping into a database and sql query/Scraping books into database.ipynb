{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from sqlite3 import connect\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(url = \"https://books.toscrape.com/index.html\"):\n",
    "    \"\"\"Read links in requests and returns a single html.parser\"\"\"\n",
    "    # First step read the url as normal links\n",
    "    libraries = soup(requests.get(url).text, \"html.parser\")\n",
    "    \n",
    "    # return html tags\n",
    "    return libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_links(libraries):\n",
    "    \"\"\"Returns lis of full links of category`s book\"\"\"\n",
    "    # Fixed domain (i.e part of link doesn`t change)\n",
    "    DomainLinks = \"https://books.toscrape.com/\"\n",
    "    FullLinks = [ ]\n",
    "    # make list for all cateogry links \n",
    "    for link in libraries.find_all(\"li\", attrs={\"class\":\"\"}):\n",
    "        FullLinks.append(DomainLinks + link.find(\"a\").get(\"href\"))\n",
    "        \n",
    "    # return that list\n",
    "    return FullLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_url_chunked(single_url = \"https://books.toscrape.com/index.html\"):\n",
    "    \"\"\"Cut a single link\"\"\"\n",
    "    # split the coming link\n",
    "    chunked = single_url.split(\"/\")[:-1]\n",
    "    page_html_link = [page for page in chunked ] \n",
    "    # combine after chunking with forward slash '/'\n",
    "    https = \"/\".join(page_html_link) + \"/\"\n",
    "    \n",
    "    # return parts of https link\n",
    "    return https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(single_page):\n",
    "    \"\"\"expected html tags\"\"\"\n",
    "    while single_page != '':\n",
    "        if single_page.find(\"ul\", attrs = {\"class\":\"pager\"}):\n",
    "            # Try if single_page could used 'find' on it!\n",
    "            try:\n",
    "                return single_page.find(\"ul\", attrs = {\"class\":\"pager\"}).find(\"li\", attrs={\"class\":\"next\"}).find(\"a\").get(\"href\")\n",
    "            # Catch errors and break the loop in display_scarping\n",
    "            except Exception as e:\n",
    "                break\n",
    "        else:\n",
    "            # returning \"\" if there`s no next page to scrap\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_merge(part1, part2):\n",
    "    \"\"\"Mergeing url the fixed part and nexts part\"\"\"\n",
    "    if part2 != None:\n",
    "        # return mergeed URL-> links webpage\n",
    "        return part1 + part2\n",
    "    else:\n",
    "        # returning \"\" if there`s no next page to scrap\n",
    "        return part1 + \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting on Database Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_on_tables(page_html):\n",
    "    \"\"\"write on books table the following details (names, rates, prices)\n",
    "    the type of category comes from outside function write_on_cat_table()\"\"\"\n",
    "    \n",
    "    numerical_rates = {\"One\":1, \"Two\":2, \"Three\":3, \"Four\":4, \"Five\":5}\n",
    "    category = page_html.find(\"title\").get_text().split(\"|\")[0].strip()\n",
    "    \n",
    "    for row in page_html.find_all(\"article\" , attrs= {\"class\":\"product_pod\"}):\n",
    "        names = row.find(\"h3\").find(\"a\").get(\"title\")\n",
    "        rates = numerical_rates[row.find(\"p\").get(\"class\")[1]]\n",
    "        prices = float(row.find_all(\"p\", attrs={\"class\":\"price_color\"})[0].get_text()[2:])\n",
    "\n",
    "        connection.execute(\"INSERT INTO books (book_name, rates, price) values (?,?,?)\", [names, rates, prices]) \n",
    "        connection.commit()\n",
    "         \n",
    "        book_id = connection.execute(\"SELECT book_id FROM books\").fetchall()[-1][0]\n",
    "        wite_on_cat_table(category, book_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wite_on_cat_table(category, identity_book):\n",
    "    \"\"\"write type of category of specific book into categories table\"\"\"\n",
    "    connection.execute(\"INSERT INTO categories (category, book_id) values (?, ?)\", [category, identity_book])\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function \"Displaying scraping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scraping(first_reading):\n",
    "    \"\"\"The main function starts scraping\"\"\"\n",
    "    for single_link in get_category_links(first_reading):\n",
    "        step2 = read_html(single_link)\n",
    "        write_on_tables(step2)\n",
    "\n",
    "        lp_step3 = get_next_page(step2)\n",
    "        fp_step4 = make_single_url_chunked(single_link)\n",
    "\n",
    "        step5 = url_merge(fp_step4, lp_step3)\n",
    "\n",
    "        while get_next_page(step2):\n",
    "            lp_step3 = get_next_page(step2)\n",
    "            fp_step4 = make_single_url_chunked(single_link)\n",
    "            step5 = url_merge(fp_step4, lp_step3)\n",
    "\n",
    "            step2 = read_html(step5)\n",
    "            write_on_tables(step2)\n",
    "    print(\"The webpage has been scraped into database 'library' successfully \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage has been scraped into database 'library' successfully \n"
     ]
    }
   ],
   "source": [
    "# connect Database -> 'library'\n",
    "connection = connect(\"library.db\")\n",
    "\n",
    "# 2 functions for scraping and recording on DB\n",
    "first_reading = read_html()\n",
    "display_scraping(first_reading)\n",
    "\n",
    "# close DB after recording on it\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
