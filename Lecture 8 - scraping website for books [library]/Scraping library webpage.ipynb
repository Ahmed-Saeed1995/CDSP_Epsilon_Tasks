{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1\n",
    "--------------\n",
    "Scrap the books(title, price, rate,Category) and put them into a csv file\n",
    "https://books.toscrape.com/index.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(url = \"https://books.toscrape.com/index.html\"):\n",
    "    \"\"\"Read links in requests and returns a single html.parser\"\"\"\n",
    "    # First step read the url as normal links\n",
    "    libraries = soup(requests.get(url).text, \"html.parser\")\n",
    "    \n",
    "    # return html tags\n",
    "    return libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_links(libraries):\n",
    "    \"\"\"Returns lis of full links of category`s book\"\"\"\n",
    "    # Fixed domain (i.e part of link doesn`t change)\n",
    "    DomainLinks = \"https://books.toscrape.com/\"\n",
    "    FullLinks = [ ]\n",
    "    # make list for all cateogry links \n",
    "    for link in libraries.find_all(\"li\", attrs={\"class\":\"\"}):\n",
    "        FullLinks.append(DomainLinks + link.find(\"a\").get(\"href\"))\n",
    "        \n",
    "    # return that list\n",
    "    return FullLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_url_chunked(single_url = \"https://books.toscrape.com/index.html\"):\n",
    "    \"\"\"Cut a single link\"\"\"\n",
    "    # split the coming link\n",
    "    chunked = single_url.split(\"/\")[:-1]\n",
    "    page_html_link = [page for page in chunked ] \n",
    "    # combine after chunking with forward slash '/'\n",
    "    https = \"/\".join(page_html_link) + \"/\"\n",
    "    \n",
    "    # return parts of https link\n",
    "    return https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page(single_page):\n",
    "    \"\"\"expected html tags\"\"\"\n",
    "    while single_page != '':\n",
    "        if single_page.find(\"ul\", attrs = {\"class\":\"pager\"}):\n",
    "            # Try if single_page could used 'find' on it!\n",
    "            try:\n",
    "                return single_page.find(\"ul\", attrs = {\"class\":\"pager\"}).find(\"li\", attrs={\"class\":\"next\"}).find(\"a\").get(\"href\")\n",
    "            # Catch errors and break the loop in display_scarping\n",
    "            except Exception as e:\n",
    "                break\n",
    "        else:\n",
    "            # returning \"\" if there`s no next page to scrap\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_merge(part1, part2):\n",
    "    \"\"\"Mergeing url the fixed part and nexts part\"\"\"\n",
    "    if part2 != None:\n",
    "        # return mergeed URL-> links webpage\n",
    "        return part1 + part2\n",
    "    else:\n",
    "        # returning \"\" if there`s no next page to scrap\n",
    "        return part1 + \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_header():\n",
    "    \"\"\"This function didicated to wite only header on file before calling scraping function\"\"\"\n",
    "    # open the file and wite header\n",
    "    with open(\"Books.csv\", \"a+\", newline='',encoding='utf-8' ) as f:\n",
    "        file = csv.writer(f)\n",
    "        # the names of each column (i.e the header of whole informations)\n",
    "        file.writerow([\"Book-Name\", \"Rates\", \"Price\", \"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_on_file(page_html):\n",
    "    \"\"\"This function store scraped infromation on csv file\"\"\"\n",
    "    # make dictionary to convert strings into ordinal numbers\n",
    "    numerical_rates = {\"One\":1, \"Two\":2, \"Three\":3, \"Four\":4, \"Five\":5}\n",
    "    \n",
    "    # open a file in append position to write on it\n",
    "    with open(\"Books.csv\", \"a+\", newline='',encoding='utf-8' ) as f:\n",
    "        file = csv.writer(f)\n",
    "\n",
    "        # fet the name of current category\n",
    "        book_category = page_html.find(\"title\").get_text().split(\"|\")[0].strip()\n",
    "        \n",
    "        # loop in whole \"HTML tags\" to scraping and store information on csv file\n",
    "        for row in page_html.find_all(\"article\" , attrs= {\"class\":\"product_pod\"}):\n",
    "            \n",
    "            # assign variables scraped infromation and to save it on csv file later\n",
    "            book_names = row.find(\"h3\").find(\"a\").get(\"title\")\n",
    "            book_rates = numerical_rates[row.find(\"p\").get(\"class\")[1]]\n",
    "            book_prices = float(row.find_all(\"p\", attrs={\"class\":\"price_color\"})[0].get_text()[2:])\n",
    "            \n",
    "            # save all information that has been scraped to csv file according to thier header`s name\n",
    "            file.writerow([book_names, book_rates, book_prices, book_category])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function \"Displaying scraping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scraping(first_reading):\n",
    "    \"\"\"The main function starts scraping\"\"\"\n",
    "    for single_link in get_category_links(first_reading):\n",
    "        step2 = read_html(single_link)\n",
    "        write_on_file(step2)\n",
    "\n",
    "        lp_step3 = get_next_page(step2)\n",
    "        fp_step4 = make_single_url_chunked(single_link)\n",
    "\n",
    "        step5 = url_merge(fp_step4, lp_step3)\n",
    "\n",
    "        while get_next_page(step2):\n",
    "            lp_step3 = get_next_page(step2)\n",
    "            fp_step4 = make_single_url_chunked(single_link)\n",
    "            step5 = url_merge(fp_step4, lp_step3)\n",
    "\n",
    "            step2 = read_html(step5)\n",
    "            write_on_file(step2)\n",
    "    print(\"The webpage has been scrapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_header()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage has been scrapped\n"
     ]
    }
   ],
   "source": [
    "first_reading = read_html()\n",
    "display_scraping(first_reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
